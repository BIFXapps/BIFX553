---
title: "BIFX 553 - Discussion 3"
author: "Randy Johnson"
date: "February 2, 2017"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, error = TRUE, fig.width = 4, fig.asp = 1)
library(car)
library(tidyverse)
library(broom)
theme_set(theme_classic() +
          theme(axis.line.x = element_line(color = 'black'),
                axis.line.y = element_line(color = 'black')))
```

Assessing Model Fit and Assumptions
===================================

Regression Assumptions
----------------------

We will primarily use visual inspection and the `car` package for checking regression assumpitions, but there are many other resources in R to do this (e.g. the `rms` and `gvlma` packages).

- Linear relationship
- Multivariate Normality
- No/little multicollinearity
- No autocorrelation
- Homoscedasticity

How does our model hold up?

```{r visually check linear assumption}
load('../1-26/gbsg.RData')

# model from Discussion2
gbsg.lm <- lm(nodes ~ age + size + grade + meno + pgr + er + hormon, data = gbsg)

ggplot(gbsg, aes(size, nodes)) +
  geom_point() +
  geom_smooth(method = 'lm')
```

[//]: # (\textcolor{red}{Size and nodes are integers, so we aren't really sure how they stack up. Use jitter and transparency.})

[//]: # \textcolor{red}{ggplot(gbsg) +}
[//]: # \textcolor{red}{  geom\_jitter(alpha = 0.2) +}
[//]: # (\textcolor{red}{  geom\_smooth(method = 'lm') +})
[//]: # \textcolor{red}{  geom\_rug() +}
[//]: # \textcolor{red}{  scale\_y\_log10() +}
[//]: # \textcolor{red}{  scale\_x\_log10()}

```{r new model, include=FALSE}
gbsg.lm2 <- lm(nodes ~ age + size + grade + meno + pgr + er + hormon, data = gbsg)
```

### Linear Relationship

*Component residual plots* are a way to see if the predictors have a linear relationship with the outcome variable. The red, dashed line in the figures below represents the best fit of each preidictor and the residuals, and the solid, green line is a running, smoothed average along the x-axis. In this example,

```{r linearity example, fig.width=5, fig.asp = .5}
tmp <- data_frame(x1 = rnorm(100),
                  x2 = rnorm(100),
                  y = x1 + x2^2 + rnorm(100))
lm(y ~ x1 + x2, data = tmp) %>%
  crPlots()
```

How does our model look?

```{r linearity check, fig.width=7}
crPlots(gbsg.lm2)
```

### Multivariate Normality

There are a couple of ways we can look at normality. The Sapiro-Wilk test for normality will give you a quantitative measure of whether your residuals are normally distributed, and the QQ plot will give you a graphical way to see what is going on with your residuals.

```{r normality check}
# Shpiro-Wilk test for normality
shapiro.test(augment(gbsg.lm2)$.std.resid)

# quantile quantile plot
qqPlot(gbsg.lm2)
```

You could also plot a histogram of your residuals along with a normal distribution.

```{r normality check2}
# take a look at the residuals
augment(gbsg.lm2) %>%
  ggplot(aes(.std.resid)) + # plot studentized residuals on x-axis
  geom_histogram(aes(y = ..density..)) + # plot histogram as density, rather than frequency
  stat_function(fun = dnorm, color = 'red') # put a N(0,1) density over the top
```

### No/little multicollinearity

*Variance inflation factors* give you a meeasure of how much the residuals are inflated when each predictor is added to the model (compared to the full model minus the predictor in question). This can give us an idea for what predictors might be correlated with eachother. Anything greater than 2 should certainly give us pause.

```{r multicolloinearity check}
# variance inflation factors
vif(gbsg.lm2)
```

### No autocorrelation

*Autocorrelation* most often occurs when there is a correlation between observations at regular time intervals.The Durbin-Watson test will give us a measure of autocorrelation (the null hypothesis is that there is no autocorrelation).

[//]: # \textcolor{red}{For example, daylight cycles. Draw picture.}

```{r autocorrelation check}
durbinWatsonTest(gbsg.lm2)
```

### Homoscedasticity

*Heteroscedasticity* refers to a model where the variance about the predicted value of the outcome changes as the predicted value increases. We can check for this using a score test for non-constant error variance or graphically.

[//]: # \textcolor{red}{Draw a picture.}

```{r homoscedasticity check}
# statistical test for heteroscedasticity (null is that they are homoscedastic)
ncvTest(gbsg.lm2)

# scale-location plot
plot(gbsg.lm2, which = 3)
```

The `spreadLevelPlot` function gives us a slightly nicer picture of this. The solid red line is what we would hope to see under a homoscedastic model, and the green line is what is actually observed. These two plots (the one just above and the one just below) are represenations of the exact same data. The plot below, however, is scaled differently to highlight departures from the expected.

```{r homo check2}
# another scale-location plot
spreadLevelPlot(gbsg.lm2)
```

### Outliers / Influential Points

The `outlierTest` function gives us a list of the most significant outliers in a model by row number. The maxumum number of rows to return is controlled by the `n.max` function argument.

```{r Outlier Tests1}
# returns the most significant outliers in the residuals
outlierTest(gbsg.lm2)
```

We can also graphically inspect the influence that each row of data (or each individual in the sample) has on the overall model. Cook's distance gives us a measure of how much each row of data influences the model. Ideally we would like them all to be close to the same, but we sometimes have some values that are overly influential (see [this illustration](https://www.youtube.com/watch?v=zi5kuvg9Aho) of how one data point can influence a regression fit).

[//]: # (\textcolor{red}{The hat values come from the diagonal of the hat matrix:})
[//]: # (\textcolor{red}{$$\hat y = Xb = X\left( X^T X \right)^{-1} X^T y$$})

```{r Outlier Tests2}
# Cook's Distance
plot(gbsg.lm2, which = 4)

influencePlot(gbsg.lm2)
```

### Influence of terms in the model

We also want to make our model as parsemonious as we can. Sometimes we will include a prediction variable because we believe that it is important, but usually we will include preditors only if they appear to be statistically important to our model. To get a quick look at the statistical significance of a predictor, we can just look at the regression output.

```{r regression output}
tidy(gbsg.lm2)
```

*Added Vairiable Plots* provide a graphical approach to identifying how much added predictive value a specific variable will give you. In order to generate an Added Variable Plot, we need to perform two additional regressions.

```{r avPlot example}
tmp <- data_frame(x1 = rnorm(100),
                  x2 = rnorm(100),
                  x3 = rnorm(100),
                  x4 = rnorm(100),
                  y = x1 + x2 + x3 + rnorm(100))

# full model
full_model <- lm(y ~ x1 + x2 + x3 + x4, data = tmp)
tidy(full_model)

# model without x1
no_x1 <- update(full_model, . ~ . - x1) %>%
  augment()

no_x1$x1 <- tmp$x1 # put back into the data_frame, but not into the model

# now regress x1 onto y conditioning on the other variables
ggplot(data = no_x1, aes(x = x1, y = .resid)) +
  geom_point() +
  geom_smooth(method = 'lm', color = 'red')
```

Now lets take a look at all of the predictors in our model:

```{r term influence, fig.width=7}
# added variable plots
avPlots(gbsg.lm2)
```

[//]: # (\textcolor{red}{This doesn't look so great, so let's update our model (in the new model code chunk above) to something that makes more sense!})

## Homework

Choose a model as a team for the clinical data in Project 1, checking all model assumptions. Each individual should submit their own description justifying your group's model choice. **Limit: 500 words.** Figures and tables may be included if they add to the discussion. If you want to refer to R code, you may include a link to your repository or to a Gist, but do not include the R code in your write-up.
