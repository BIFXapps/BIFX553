---
title: "BIFX 553 - Model Building Review"
author: "Randy Johnson"
date: "February 23, 2017"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(error = TRUE, fig.width = 4, fig.asp = 1)
library(shrink)
library(car)
library(tidyverse)
library(broom)
theme_set(theme_classic() +
          theme(axis.line.x = element_line(color = 'black'),
                axis.line.y = element_line(color = 'black'),
                text = element_text(size = 15)))
```

## Setup
\small
```{r setup visible, eval = FALSE}
library(shrink)
library(car)
library(tidyverse)
library(broom)
theme_set(theme_classic() +
          theme(axis.line.x = element_line(color = 'black'),
                axis.line.y = element_line(color = 'black'),
                text = element_text(size = 15)))
```

##
\Large
Multivariable Modeling Strategies

## Prespecification of predictor complexity

- Deciding on a model before analysis of the data is the best policy when model building. Unless you have a good reason to remove something from the model after beginning the analysis, you probably shouldn't.

- If you don't know much about the system you are modelling, decide before hand how complex you want your model to be.

## Variable selection

There are automated methods for building your model (e.g. forward or backward selection). These are best avoided because:

- $r^2$ values will be biased too high.
- Test statistics do not have the correct distribution.
- Standard errors / confidence intervals are biased smaller than they should be.
- p-values will be too small
- Collinearity makes variable selection arbitrary
- It allows you to turn off your brain.

## Overfitting

Over-fitting is likely to occur if the number of parameters is too high.

At some point, new parameters added to the model will fit sampling noise rather than the information we are trying to make inferences about.

A common rule of thumb for picking the number of variables is $\frac{sample~size}{10}$ or $\frac{sample~size}{20}$ for continuous variables (see pg 61 of Harrell, 2001 for more information).

For genetic markers, this is more like $\frac{MAF}{10}$ or $\frac{MAF}{20}$, where MAF = minor allele frequency.

## Regression to the Mean

When using a model to predict outcomes in independent data, predicted values that are far away from the mean will tend to over estimate the deviation from the mean, while predicted values near the mean will tend to be more accurate.

\small
```{r regression to the mean1}
set.seed(28347) # this is the first seed I tried
n_pred <- 100
pred <- data_frame(x1 = rnorm(n_pred),
                   x2 = rnorm(n_pred),
                   x3 = rnorm(n_pred),
                   x4 = rnorm(n_pred),
                   x5 = rnorm(n_pred),
                   y = x1 + x2 + x3 + rnorm(n_pred))

model <- lm(y ~ x1 + x2 + x3 + x4 + x5, 
            data = pred, y = TRUE, x = TRUE)
```

## Regression to the Mean
\small
```{r regression to the mean2}
tidy(model)
```

## Shrinkage
The regression coefficient for $x5$ is too big, and the coefficients for $x1$, $x2$ and $x3$ are all slightly too big. Predictions of $y$ using this model will be slightly too big, and this effect will be magnified for points lying further from the mean.

Shrinkage can be used to compensate some for this (models with more parameters will benefit more).

```{r shrinkage}
shrink(model)
```

## Collinearity

- Standard errors will be inflated.
- Correlated coefficients are difficult to estimate, because data provide limited information when holding another, correlated parameter constant.
- It is difficult to sort out the important variable(s)

## Data reduction

Using what is known about the system you are modelling can be on of the best ways to reduce the number of parameters. Read the literature / consult experts.

Clustering or summarizing (e.g. using principal components) can be another powerful tool to reduce the dimensionality of your data (more on this to come in future discussions).

## Overly influential variables

Sources include:

- Data entry errors
- Measurement error (if a particular variable is too noisy, it may be best to just throw out all observations of that particular predictor)
- Data need to be transformed
- Not enough data to model a complex system
- Model doesn't fit a specific subset of the data

## Comparing two models

- Consider model assumptions / all of the above.
- A smaller model isn't always preferred if important statistically non-significant variables are omitted.
- Does it agree with our understanding of the system (e.g. our disease model)?

## Summary: Developing predictive models

- Collect lots of data.
- Do your homework when developing the disease model/hypothesis.
- Impute missing data if there isn't much. Will people using your model have trouble collecting all the variables you are using?
- Reduce the number of predictors if necessary/practical.
- Check all assumptions.
- Influential observations are especially bad in predictive models.
- Validate your final model.
- Use parameter shrinkage if possibility of over fitting.
- Develop some simplified alternative models.

## Summary: Developing models for effect estimation

- Parsimony isn't as important in this context.
- Interactions should still receive careful consideration.
- Imputation of predictors is less helpful.
- Check all assumptions.

## Summary Developing models for hypothesis testing

- Check all assumptions.
- Interactions should still receive careful consideration.
- Imputation might be helpful, but often results in no net benefit.

##
\Large
Assumptions Review

## Example data
\small
```{r bad data}
set.seed(923747)
n <- 500
dat <- data_frame(x = rnorm(n),
                  x2 = rnorm(n, x, 0.5),
                  x3 = rnorm(n),
                  x4 = 1:n,
                  y0 = x + x3 + rnorm(n),
                  nonlin_y = x^2 + rnorm(n),
                  auto_y = diffinv(rnorm(n-1)),
                  homo_y = x + rnorm(n, sd = (x + abs(min(x)))),
                  out_y = y0) %>%
       bind_rows(data_frame(x = c(0,4), out_y = rep(10,2)))
  

dat$x[n-1] <- max(dat$x) # outlier with leverage
dat$x[n] <- 0 # outlier without leverage

# OK
model0 <- lm(y0 ~ x, data = dat)
```

## Example data

```{r models}
# Nonlinear + Non-normal
model1 <- lm(nonlin_y ~ x, data = dat)

# Multicollinearity
model2 <- lm(y0 ~ x + x2 + x3, data = dat)
model3 <- update(model2, . ~ . - x2) # OK

# Autocorrelation
model4 <- lm(auto_y ~ x4, data = dat)

# Homoscedasticity
model5 <- lm(homo_y ~ x, data = dat)

# with outliers
model6 <- lm(out_y ~ x, data = dat)
```

## Linearity

Component Residual Plots, `crPlots()`, shows the relationship between each predictor and the outcome, after accounting for all of the other variables in the model. Since this doesn't work so well with interactions, simply remove the interactions to view these relationships.
```{r linearity, echo=FALSE, fig.width=3, fig.heigt=3.5}
options(warn=FALSE)

crPlots(model0, ylab = '', xlab = 'model0')
crPlots(model1, ylab = '', xlab = 'model1')
```

## Multivariate Normality

There are a few ways to check for multivariate normality.

- `shapiro.test()`
- `qqPlot()`
- Plotting a histogram of the residuals (redundant)

```{r normality, echo=FALSE, fig.width=2.8, fig.height=3.5, fig.align='center', fig.show='hold'}

p0 <- shapiro.test(augment(model0)$.std.resid)$p %>%
      signif(2)
p1 <- shapiro.test(augment(model1)$.std.resid)$p %>%
      signif(2)


qqPlot(model0, ylab = '', xlab = paste0('model0 (p=', p0, ')'))
qqPlot(model1, ylab = '', xlab = paste0('model1 (p=', p1, ')'))
```

## Little/No Multicollinearity

If the variance inflation factors are too big (e.g. greater than 2), they are likely to be collinear with another predictor. Removing one of the collinear variables is usually sufficient to meet this assumption.

```{r multicollinearity}
vif(model2)
vif(model3)
```

## No Autocorrelation

Autocorrelation occurs when there is correlation between the elements of a series that are separated by a given interval (e.g. 1 day).

```{r autocorrelation, echo=FALSE, fig.width=6, fig.asp=0.3, fig.align='center'}
options(warn = -1)
ggplot(dat, aes(x4, auto_y)) +
  geom_point() + 
  geom_smooth(se = FALSE, span = 0.1, method = 'loess')

durbinWatsonTest(model4)
```

## Homoscedasticity

Heteroscedasticity occurs when the variance about the regression line is not constant. We can test this assumption with `ncvTest()` or `spreadLevelPlot()`.

```{r homoscedasticity1, echo=FALSE, fig.width=6, fig.asp=0.3, fig.align='center'}
options(warn = -1)
ggplot(dat, aes(x, homo_y)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)

ncvTest(model5)
```

## Homoscedasticity

We want both trend lines to be as flat as possible (i.e. straight line with slope = 0).

```{r homoscedasticity2, echo=FALSE, fig.width=2.8, fig.height=3, fig.align='center', fig.show='hold'}
options(warn=-1)

spreadLevelPlot(model0, ylab = '') %>%
  invisible()

spreadLevelPlot(model5, ylab = '') %>%
  invisible()
```

## Outliers and Leverage

Use `outlierTest()` to get a list of outliers. 

```{r outliers}
outlierTest(model6)
```

Remember that some outliers affect our model more than others. `influencePlot()` will give us a nice picture of which outliers we need to worry about.

## Outliers and Leverage

- Hat-values: A measure of the amount of influence each data point has on the outcome predictions.

- Residuals: The difference between the observed and predicted value of the outcome variable.

- Studentized residuals: Scaled residuals, such that they have mean = 0 and variance = 1.

- Cook's distance: A measure of the effect of each data point on the regression coefficients.

- `influencePlot()` gives reference lines at
    - vertical: `mean(.hat)*c(2,3)`
    - horizontal: 0 and $\pm 2$ SDs

## Outliers and Leverage

```{r leverage, echo = FALSE}
influencePlot(model6) %>%
  invisible()
```

## Outliers and Leverage

```{r leverage2, echo=FALSE}
ggplot(dat, aes(x, out_y)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE) +
  annotate('text', x=c(4,4,0), y=c(1,9.5,9.5), label = c(499,502,501))
```