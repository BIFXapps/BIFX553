---
title: "Generalized Logistic Regression"
author: "Randy Johnson"
date: "3/2/2017"
header-includes:
  \usepackage{cancel}
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(broom)
theme_set(theme_classic() +
          theme(axis.line.x = element_line(color = 'black'),
                axis.line.y = element_line(color = 'black'),
                text = element_text(size = 15)))
```

##
\Large
Generalized Linear Regression

## GLM

Generalized Linear Models (GLMs) allow us to relax the multivariate normality assumption.

Assumptions:

- Model fit is correct
- No/little multi-collinearity
- No overly influential variables

\scriptsize
```{r glm args, echo=TRUE}
args(glm)
```

## Linear Regression
- Response: Continuous
- Family: gaussian(link = 'identity')
- Interpretation:
    - $\beta_0$ is the mean response in the reference group (Intercept).
    - $\beta_{1 \ldots n}$ are the mean differences comparing those exposed to $X$ with the reference group.
- Caveat: Same as if we used `lm()`, so the multivariate normality assumption should hold.

$$ \mbox{E}(Y|X) = \beta_0 + \beta_1X_1 + \cdots + \beta_nX_n $$

## Log-linear Regression
- Response: Continuous, Time to event, Count
- Family: poisson(link = 'log')
- Interpretation:
    - $\beta_0$ is the log mean response in the reference group.
    - $\beta_{1 \ldots n}$ are the log ratios of the means comparing those exposed to $X$ with the reference group.
- Caveat: `mean(y)` is assumed to be equal to `var(y)` -- use quasipoisson(link = 'log') in the event that this assumption is not valid.
    
$$ \log \mbox{E}(Y|X) = \beta_0 + \beta_1X_1 + \cdots + \beta_nX_n $$

## Logistic Regression
- Response: Binary
- Family: binomial(link = 'logit')
- Interpretation:
    - $\beta_0$ is the log odds for the reference group (cohort study only).
    - $\beta_{1 \ldots n}$ are the log odds ratios (ORs) comparing those exposed to $X$ with the reference group. These are sometimes referred to as LOD scores.
- Caveats:
    - $\beta_0$ doesn't have any real-world interpretation for case-control studies.
    - The OR will slightly over estimate the Relative Risk Ratio. For rare disease this difference is negligible.
    
$$ \log odds(Y|X) = \beta_0 + \beta_1X_1 + \cdots + \beta_nX_n $$

## Odds vs Relative Risk

Given $A$ equal to the number of events and $B$ equal to the number of non-events,

\begin{columns}
\begin{column}{0.48\textwidth}
$$
\begin{aligned}
odds(Y|X) &= \frac{\mbox{P}(Y|X)}{1 - \mbox{P}(Y|X)} \\
          &= \frac{\frac{A}{\cancel{A+B}}}{\frac{B}{\cancel{A+B}}} \\
          &= \frac{A}{B}
\end{aligned}
$$
\end{column}
\begin{column}{0.48\textwidth}
$$
\begin{aligned}
RR(Y|X) &= \mbox{P}(Y|X) \\
        &= \frac{A}{A+B}
\end{aligned}
$$
\end{column}
\end{columns}

Thus one of the major advantages of modeling the odds is that you don't need to know the prevalence or incidence in the population. This is why we use Odds Ratios in case control studies instead of RR Ratios.

## Odds vs Relative Risk

The odds will always overestimate the relative risk,

$$
\begin{aligned}
odds(Y|X)   &> RR(Y|X) \\
\frac{A}{B} &> \frac{A}{A+B} \\
\frac{A}{B} & \begin{array}{c}
                ? \\[-14pt]
                \approx
              \end{array}
              \frac{A}{A+B}
\end{aligned}
$$

but they will be approximately equal if the number of events, $A$, is small relative to $B$ (i.e. when the event is rare). How rare does the event need to be?

## Odds vs Relative Risk

```{r rare events, echo = FALSE}
rare_events <- data_frame(RR = exp(seq(from = log(0.01), 
                                       to = log(0.2), 
                                       length = 300)), # relative risk
                          odds = exp(log(RR) - log(1 - RR)))

ggplot(rare_events, aes(RR, odds - RR)) +
  geom_line() +
  scale_x_log10() +
  geom_hline(yintercept = 0.01, linetype = 2) + 
  geom_vline(xintercept = 0.1, linetype = 2)
```

## Log-binomial Regression
- Response: Binary
- Family: binomial(link = 'log')
- Interpretation:
    - $\beta_0$ is the log incidence/prevalence for the reference group.
    - $\beta_{1 \ldots n}$ are the log incidence/prevalence ratios comparing those exposed to $X$ with the reference group.
- Caveat:
    - This model only makes sense in the context of cohort studies.
    - It tends to have more stability issues than logistic regression.

$$ \log \mbox{P}(Y|X) = \beta_0 + \beta_1X_1 + \cdots + \beta_nX_n $$
